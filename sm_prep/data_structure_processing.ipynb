{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(cwd,'Package_122816/abcd_ant01.txt')\n",
    "\n",
    "df = pd.read_csv(fp,sep='\\t')\n",
    "df.drop(index=0,inplace=True)\n",
    "num_cols = len(list(df.columns[9:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only want to take the earliest record for each subject, we need to\n",
    "# 1. First convert the 'interview_date' column to a pandas DateTime type\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "\n",
    "# Next, we need to remove the redundant entries for each subject\n",
    "# 1. order the records chronologically (in order of early --> later)\n",
    "df_1 = df.sort_values(by='interview_date')\n",
    "\n",
    "# 2. Drop duplicate records (based on subjectkey), keeping the first occurance (i.e. temporally first, due to chron. ordering)\n",
    "df_1 = df_1.drop_duplicates(subset='subjectkey',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(11875, 23)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Unable to parse string \"Arm\" at position 339\n1 could not convert column anthro_weight_a_location\n\nUnable to parse string \"2017-07-24 19:18\" at position 1\n1 could not convert column anthro_timestamp\n\n['anthro_weight_a_location', 'anthro_timestamp']\n"
    }
   ],
   "source": [
    "# Next, convert values to numeric and categorical\n",
    "categorical_cols = []\n",
    "# Convert columns to numeric values where possible\n",
    "# Keep track of cols that fail so they can be made into categorical data\n",
    "for col in df_1.columns[9:-2]:\n",
    "    try:\n",
    "        df_1[col] = pd.to_numeric(df_1[col], errors='raise')\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(\"1 could not convert column {}\\n\".format(col))\n",
    "        categorical_cols.append(col)\n",
    "\n",
    "print(categorical_cols)\n",
    "\n",
    "# Following line needed before categorical casting\n",
    "df_1[categorical_cols] = df_1[categorical_cols].fillna(value='MISSING')\n",
    "\n",
    "# # Now, try to convert remaining columns to categories (only the ones which couldnt convert to numeric)\n",
    "for col in df_1.select_dtypes(exclude=['number']).columns[9:-2]:\n",
    "    # for col in categorical:\n",
    "    try:\n",
    "        df_1[col] = pd.Categorical(df_1[col], categories=df_1[col].unique()).codes\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(\"2 could not convert column {}\\n\".format(col))\n",
    "\n",
    "# print(df.dtypes)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "collection_id                       object\nabcd_ant01_id                       object\ndataset_id                          object\nsubjectkey                          object\nsrc_subject_id                      object\ninterview_date              datetime64[ns]\ninterview_age                       object\ngender                              object\neventname                           object\nanthro_1_height_in                  object\nanthro2heightin                     object\nanthro3heightin                     object\nanthroheightcalc                    object\nanthroweightcast                    object\nanthro_weight_a_location            object\nanthroweight1lb                     object\nanthroweight2lb                     object\nanthroweight3lb                     object\nanthroweightcalc                    object\nanthro_waist_cm                     object\nanthro_timestamp                    object\ncollection_title                    object\nstudy_cohort_name                   object\ndtype: object"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "anthro_1_height_in\n649\nCOLUMN: anthro_1_height_in\n\nLargest equal vals group:\t649\nnum nan values:\t11\nnum not nan values:\t11864\n\nanthro2heightin\n643\nCOLUMN: anthro2heightin\n\nLargest equal vals group:\t643\nnum nan values:\t9\nnum not nan values:\t11866\n\nanthro3heightin\n16\nCOLUMN: anthro3heightin\n\nLargest equal vals group:\t16\nnum nan values:\t11491\nnum not nan values:\t384\n\nanthroheightcalc\n594\nCOLUMN: anthroheightcalc\n\nLargest equal vals group:\t594\nnum nan values:\t9\nnum not nan values:\t11866\n\nanthroweightcast\n11321\nCOLUMN: anthroweightcast\n\nLargest equal vals group:\t11321\nnum nan values:\t527\nnum not nan values:\t11348\n\nanthro_weight_a_location\n11850\nCOLUMN: anthro_weight_a_location\n\nLargest equal vals group:\t11850\nnum nan values:\t0\nnum not nan values:\t11875\n\nanthroweight1lb\n192\nCOLUMN: anthroweight1lb\n\nLargest equal vals group:\t192\nnum nan values:\t11\nnum not nan values:\t11864\n\nanthroweight2lb\n188\nCOLUMN: anthroweight2lb\n\nLargest equal vals group:\t188\nnum nan values:\t12\nnum not nan values:\t11863\n\nanthroweight3lb\n14\nCOLUMN: anthroweight3lb\n\nLargest equal vals group:\t14\nnum nan values:\t10779\nnum not nan values:\t1096\n\nanthroweightcalc\n184\nCOLUMN: anthroweightcalc\n\nLargest equal vals group:\t184\nnum nan values:\t11\nnum not nan values:\t11864\n\nanthro_waist_cm\n910\nCOLUMN: anthro_waist_cm\n\nLargest equal vals group:\t910\nnum nan values:\t17\nnum not nan values:\t11858\n\nanthro_timestamp\n1213\nCOLUMN: anthro_timestamp\n\nLargest equal vals group:\t1213\nnum nan values:\t0\nnum not nan values:\t11875\n\n['anthro3heightin', 'anthroweightcast', 'anthro_weight_a_location', 'anthroweight3lb']\n"
    }
   ],
   "source": [
    "badcols=[]  # List to store the names of columns where either >50% data missing, or >95% values the same\n",
    "# Iterate over the columns and check these crieria, store bad columns in the badcols list\n",
    "\n",
    "num_rows = df_1.shape[0]\n",
    "\n",
    "for col in df_1.columns[9:-2]:\n",
    "    tmp = list(df_1[col].value_counts())  # find size of equal-values groups (ignoring NaNs)\n",
    "    print(col)\n",
    "    print(tmp[0])\n",
    "    largest_eq_val_group = tmp[0]               # size of largest equal-value group\n",
    "    num_nan = df_1[col].isnull().sum()            # Count how many NaNs there are\n",
    "    num_not_nan = df_1[col].count()               # Count how many rows do NOT have NaNs\n",
    "\n",
    "    print(\"COLUMN: {}\\n\".format(col))\n",
    "    print('Largest equal vals group:\\t{}\\nnum nan values:\\t{}\\nnum not nan values:\\t{}\\n'.format(largest_eq_val_group, num_nan, num_not_nan))\n",
    "\n",
    "    # 1. more than 50% missing?\n",
    "    if ( (num_nan/num_rows)>0.5 ):\n",
    "        badcols.append(col)\n",
    "        continue\n",
    "    # 2. largest equal-values group is >95% of entries?\n",
    "    elif ( (largest_eq_val_group/num_not_nan)>0.95 ):\n",
    "        badcols.append(col)\n",
    "\n",
    "print(badcols)\n",
    "\n",
    "# Drop the bad columns from the dataframe\n",
    "df_2 = df_1.drop(columns=badcols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(11875, 19)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}